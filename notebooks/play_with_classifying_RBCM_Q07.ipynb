{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c6db84-63d9-4c2a-a1e3-5a9b3464810a",
   "metadata": {},
   "source": [
    "Copyright 2025 Province of British Columbia\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28595396-7b4d-46f5-9a0b-85ee77282cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system stuff\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# the usual\n",
    "import pandas as pd\n",
    "\n",
    "# model stuff \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "# extra scores\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "\n",
    "# my stuff (abstracted non-important functions)\n",
    "# Get the project root (one level up from notebooks)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.prepare_data import get_labels_list, create_train_test_dataframes, tokenize_data, create_hf_dataset\n",
    "from src.evaluate_data import compute_metrics, classify_batch, classify_batch_llm, all_right, right_plus_extra, added_one, missed_one\n",
    "from src.config import data_path_rvm, out_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6e883-94e9-4636-a66e-0879b3afc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cores do I have to play with?\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01cbe4-ea65-4f9b-99cb-ac9aa2fb59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {ram.total / 1e9:.2f} GB\")\n",
    "print(f\"Available RAM: {ram.available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e94627-1bf4-4fe6-b35f-f40576580015",
   "metadata": {},
   "source": [
    "## Read in Data and Preprocess\n",
    "\n",
    "* Read in data\n",
    "* Create a column for use in training\n",
    "* Create a small 'training' set to mimic a human categorizing a small subset of survey responses\n",
    "* Convert to hugging face datasets for smoother processing\n",
    "* Create lists of labels to use in various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0311a-e61b-4d1a-9ee8-bfc3d2659f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_path_rvm, sheet_name = 'Q07a')\n",
    "df = df.iloc[:, 4:-1]\n",
    "df.columns = ['Response'] + list(df.columns[1:])\n",
    "labels_original = list(df.columns)[1:]\n",
    "df.columns = [x.lower().replace(' ','_').replace('/','_').replace(':','_') for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930d7b8-b1e9-489a-9964-0b71be7aa84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NANs to 0s and 'X' to 1, and add 'Other' column incase there are no X's\n",
    "df.iloc[:, 1:] = df.iloc[:, 1:].map(lambda x: 1 if x=='X' else 0)\n",
    "df['other'] = df.apply(lambda row: 0 if any(row==1) else 1, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d5a4b-4074-498d-a90d-4b1f3d73e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any NA responses\n",
    "df = df[~pd.isna(df.response)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea08db-49b7-4136-b9ce-fb42d6854af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we'll use the original labels for the LLM classifier, but mostly we want nice to work with column names \n",
    "labels_original = labels_original + ['Other']\n",
    "labels_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155818a4-9f7a-442a-a938-0148fe720c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd241035-25d7-4fa7-a506-7e96114b6bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to a multi labeled dataset for use with huggingface\n",
    "df['label'] = df.apply(get_labels_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7ce1b-802a-4dc3-a9a4-450c1891642a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create list of categories\n",
    "labels = list(df.columns[1:-1])\n",
    "display(labels)\n",
    "\n",
    "# create id2labels to use in the modelling step\n",
    "id2label = {ii: label for ii, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610db2e-ca1d-4dc6-9e8c-49ae341bbe17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e631932-196c-4b5d-ae4e-e1a85bad2286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's chunk off the majority of this data to never be used in training\n",
    "# this most matches what we would like our process to look like\n",
    "n_train = 1_000\n",
    "df_train, df_test = create_train_test_dataframes(df, n_train=n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec39dd-ac49-4a40-9de9-21baa464cc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use distillbert to tokenize the data (a smaller version of bert)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9533097-01d6-4d20-b4f1-884dcb41d441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert from pandas to a huggine face dataset to best utilize their tools\n",
    "# this will also tokenize the data\n",
    "dataset_train = create_hf_dataset(df_train, tokenizer)\n",
    "dataset_test = create_hf_dataset(df_test, tokenizer)\n",
    "dataset = create_hf_dataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49ca3d-b29f-45b1-b7fc-c28b38fc5354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train test split the data for model validation\n",
    "# the 'test' data from this step will be used as validation data in the modelling step\n",
    "dataset_train = dataset_train.train_test_split(test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853675a-ed5b-4b0d-ac4b-6c64b47aa271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ba8f5-0573-4619-91e1-63bb5ceac184",
   "metadata": {},
   "source": [
    "## Set up a model with pre-trained data\n",
    "\n",
    "* Using hugging face open source models as our starting point\n",
    "* Set up a trainer where we can play with the number of devices to use during training\n",
    "* Train the model\n",
    "* Evaluate accuracy (and test how long this step takes)\n",
    "* Save the model\n",
    "* Classify our entire dataset (and test how long this step takes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a88ea9-920f-4b70-a540-ad9ec11551e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a model for multi-label classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=len(labels),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704cae4-6740-4281-a945-5bf7c3881918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up trainer\n",
    "model_path = '../models/my_test_model_rvm'\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    # save params\n",
    "    output_dir=model_path, # save final model\n",
    "    eval_strategy=\"epoch\",        # make evaluations at end of each epoch\n",
    "    save_strategy='epoch',        # save checkpoints every epoch\n",
    "    load_best_model_at_end=True,   # save best model at end\n",
    "    \n",
    "    # learning params\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # calibrate machine params \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    \n",
    "    # logs\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train[\"train\"],\n",
    "    eval_dataset=dataset_train[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics # computes accuracy and f1 score\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfd00d-404b-474a-bcd7-e4a6ad7d6f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train our model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddb4ce-28b1-4f85-98e4-a3f58e845bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save (if didn't earlier)\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e70e2e-e5b7-4766-b382-436d1a1a0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note: you can reload this model either from a checkpoint or from the final saved model\n",
    "# # to continue training (back into a new Trainer() instance)\n",
    "\n",
    "# # examples:\n",
    "\n",
    "# # 1. CHECKPOINT \n",
    "# # load from checkpoint\n",
    "# checkpoint = f'{model_path}/checkpoint-500'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# # re-initialize trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args, # same as above\n",
    "#     train_dataset=dataset['train'],\n",
    "#     eval_dataset=dataset['test']\n",
    "# )\n",
    "\n",
    "# # resume - this preservers optimizer states, learning rate scheduler, and epoch counters\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# # 2. NEW TRAINING FROM SAVED MODEL\n",
    "# # Load the saved model (not a checkpoint)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# # New training arguments (can modify as needed)\n",
    "# new_training_args = TrainingArguments(\n",
    "#     output_dir=\"./continued_training\",\n",
    "#     # update save params, learning hyperparams, machine params, log params as you want here\n",
    "# )\n",
    "\n",
    "# # Re-initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=new_training_args,\n",
    "#     train_dataset=new_train_dataset,  # Can be the same or new dataset\n",
    "#     eval_dataset=new_val_dataset\n",
    "# )\n",
    "\n",
    "# # Start training from the loaded model\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69e56f-de48-42db-a4a8-16e862dfaaaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "results_train = trainer.evaluate(eval_dataset=dataset_train)\n",
    "results_test = trainer.evaluate(eval_dataset=dataset_test)\n",
    "print(results_train)\n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2de7e4-6d08-4957-a9d0-0ff116aeb8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "my_classifier = pipeline(\n",
    "    'text-classification', \n",
    "    model=model_path, \n",
    "    top_k=None, # needed for multi label \n",
    "    device=-1    # will use a GPU if available and set to 0\n",
    ")\n",
    "\n",
    "# some examples\n",
    "text = 'Parking is too expensive'\n",
    "#text = \"I don't live here\"\n",
    "#text = 'The ferry from vancouver is too long'\n",
    "#text = \"I just don't like the museum\"\n",
    "\n",
    "out = my_classifier(text)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f2a35-dd56-41a8-8172-c01155881696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# size of model\n",
    "n_params = my_classifier.model.num_parameters()\n",
    "n_mb = n_params * 4 / (1024**2)\n",
    "print(f'Params:    {n_params:,}\\nAppx Size: {n_mb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a24b1-1a1c-422a-9b85-88ff7bd418ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8204d40-c97b-4e0d-b7e1-aa517f7f633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify across the entire dataset\n",
    "threshold = 0.5\n",
    "batch_size = 256 # adjust based on GPU/CPU capacity \n",
    "num_proc = 4  # number of CPU cores to use \n",
    "\n",
    "# Apply inference in parallel\n",
    "result_dataset = dataset.map(\n",
    "    classify_batch,\n",
    "    fn_kwargs={'classifier': my_classifier, 'threshold': threshold, 'suffix':'_bert'},\n",
    "    batched=True,             \n",
    "    batch_size=batch_size,           \n",
    "    num_proc=num_proc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54465ec1-18ed-46c7-a596-4c31f75dc1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# back to dataframe for later analysis\n",
    "results_df = result_dataset.to_pandas().drop(['response', 'label', 'input_ids', 'attention_mask'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b878d-e96b-4f94-b0ed-6ce83ba255cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results = df.merge(results_df, left_index=True, right_index=True)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136444f-6cde-4516-a131-c354a9b764f2",
   "metadata": {},
   "source": [
    "## Set up a model with NO pre-trained data\n",
    "\n",
    "* Uses an LLM to classify data into a given set of categories\n",
    "* No pre-training required (but probably means worse outputs)\n",
    "* When setting up the pipeline can play with device value I think to make it faster/optimize our space a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3e6a3-f539-42f5-abb4-59be29202853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next build in the facebook method \n",
    "labels_original\n",
    "\n",
    "# zero shot classifier for non-trained data\n",
    "llm_classifier = pipeline(\n",
    "    'zero-shot-classification', \n",
    "    model='facebook/bart-large-mnli', \n",
    "    num_workers=1, # for debugging crashing in the DSVM (disables multiprocessing)\n",
    "    device=-1 # 0 for GPUs I think?\n",
    ")\n",
    "\n",
    "# some examples\n",
    "#text = 'Parking is too expensive'\n",
    "#text = \"I don't live here\"\n",
    "#text = 'The ferry from vancouver is too long'\n",
    "text = \"I just don't like the museum\"\n",
    "\n",
    "out = llm_classifier(\n",
    "    text, \n",
    "    # extra inputs for the LLM classifier\n",
    "    labels_original, \n",
    "    multi_label=True\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7db76-f57c-421d-a879-200ecbb94835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of model\n",
    "n_params = llm_classifier.model.num_parameters()\n",
    "n_mb = n_params * 4 / (1024**2)\n",
    "print(f'Params:    {n_params:,}\\nAppx Size: {n_mb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc1246-d618-4af7-8ce6-ac7c50991723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do across entire dataset\n",
    "threshold = 0.5\n",
    "batch_size = 256 # adjust based on GPU/CPU capacity \n",
    "num_proc = 4  # number of CPU cores to use (works on workstation)\n",
    "#num_proc = 1 # crashes for the basic DSVM, so testing different options\n",
    "\n",
    "# Apply inference in parallel\n",
    "result_dataset = dataset.map(\n",
    "    classify_batch_llm,\n",
    "    fn_kwargs={'classifier': llm_classifier, 'labels': labels_original, 'threshold': threshold, 'suffix':'_llm'},\n",
    "    batched=True,             \n",
    "    batch_size=batch_size,           \n",
    "    num_proc=num_proc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780e6fe-5709-455a-90eb-9eb271d98ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# back to dataframe for later analysis\n",
    "results_df = result_dataset.to_pandas().drop(['response', 'label', 'input_ids', 'attention_mask'], axis=1)\n",
    "results_df.columns = [x.lower().replace(' ','_').replace('/','_').replace(':','_') for x in results_df.columns]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81dda7-6dde-420b-8e00-5a336b2810b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results_complete = df_results.merge(results_df, left_index=True, right_index=True)\n",
    "df_results_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efea147-8904-48fc-8b1a-760222ff48aa",
   "metadata": {},
   "source": [
    "## Final Comparison of Outputs\n",
    "\n",
    "* Look at how well the model did at:\n",
    "  * Getting the outputs exactly matched to the initial data\n",
    "  * Getting the outputs exactly matched, but added in an extra category\n",
    "  * Adding at least one category that wasn't in initial data\n",
    "  * Missing at least one category that wasn't in initial data\n",
    "\n",
    "* More technical scores:\n",
    "  * Precision\n",
    "  * Recall\n",
    "  * F1 Score\n",
    "\n",
    "These last 3 are calculated at the micro level, to align with the language studio scoring methods.\n",
    "See [sklearn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28c93d-3a4e-4d14-82c4-33de80ac293d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels\n",
    "labels_bert = [x+'_bert' for x in labels]\n",
    "\n",
    "df_results_complete['all_right_bert'] = df_results_complete.apply(all_right, axis=1, args=(labels, labels_bert))\n",
    "df_results_complete['right_plus_extra_bert'] = df_results_complete.apply(right_plus_extra, axis=1, args=(labels, labels_bert))\n",
    "df_results_complete['added_one_bert'] = df_results_complete.apply(added_one, axis=1, args=(labels, labels_bert))\n",
    "df_results_complete['missed_one_bert'] = df_results_complete.apply(missed_one, axis=1, args=(labels, labels_bert))\n",
    "\n",
    "labels_llm = [x+'_llm' for x in labels]\n",
    "df_results_complete['all_right_llm'] = df_results_complete.apply(all_right, axis=1, args=(labels, labels_llm))\n",
    "df_results_complete['right_plus_extra_llm'] = df_results_complete.apply(right_plus_extra, axis=1, args=(labels, labels_llm))\n",
    "df_results_complete['added_one_llm'] = df_results_complete.apply(added_one, axis=1, args=(labels, labels_llm))\n",
    "df_results_complete['missed_one_llm'] = df_results_complete.apply(missed_one, axis=1, args=(labels, labels_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149055d9-039d-4c0f-9f8f-9890ac36c9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = df_results_complete[\n",
    "    [\n",
    "        'all_right_bert', 'right_plus_extra_bert', 'added_one_bert', 'missed_one_bert',\n",
    "        'all_right_llm',  'right_plus_extra_llm',  'added_one_llm',  'missed_one_llm'\n",
    "    ]\n",
    "].sum().reset_index()\n",
    "df_summary.columns = ['metric', 'n']\n",
    "df_summary['pct'] = df_summary['n']/len(df_results_complete)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0f9c5-5de4-497d-92d8-d41b5a10ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add precision, recall, f1 scores\n",
    "y = df_results_complete[labels].values.astype('float')\n",
    "y_bert = df_results_complete[labels_bert].values.astype('float')\n",
    "y_llm = df_results_complete[labels_llm].values.astype('float')\n",
    "\n",
    "extra = pd.DataFrame(\n",
    "    {'metric': ['f1_bert', 'prec_bert', 'recall_bert', 'f1_llm', 'prec_llm', 'recall_llm'], \n",
    "     'n': [len(df_results_complete)]*6, \n",
    "     'pct': [\n",
    "         f1_score(y, y_bert, average='micro'),\n",
    "         precision_score(y, y_bert, average='micro'),\n",
    "         recall_score(y, y_bert, average='micro'),\n",
    "         f1_score(y, y_llm, average='micro'),\n",
    "         precision_score(y, y_llm, average='micro'),\n",
    "         recall_score(y, y_llm, average='micro')\n",
    "     ]\n",
    "    }\n",
    "     )\n",
    "\n",
    "df_summary = pd.concat([df_summary, extra])\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d6ba5-a93f-4485-9a53-b9cef149b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af589b-9b27-4c29-86a6-3168524b3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get same results but for test data\n",
    "tmp = df_test.copy()\n",
    "tmp['test'] = 1\n",
    "df_results_complete = df_results_complete.merge(tmp[['test']], right_index=True, left_index=True, how='left')\n",
    "df_results_complete['test'] = df_results_complete['test'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "df_results_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8e46d-1124-4277-bc80-fbfe4c8aef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_test = df_results_complete[df_results_complete['test']==1][\n",
    "    [\n",
    "        'all_right_bert', 'right_plus_extra_bert', 'added_one_bert', 'missed_one_bert',\n",
    "        'all_right_llm',  'right_plus_extra_llm',  'added_one_llm',  'missed_one_llm'\n",
    "    ]\n",
    "].sum().reset_index()\n",
    "df_summary_test.columns = ['metric', 'n']\n",
    "df_summary_test['pct'] = df_summary_test['n']/(df_results_complete['test'].sum())\n",
    "df_summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ed3e2-aa57-4299-8a30-6369841e1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prec/f1/recall for test data\n",
    "# add precision, recall, f1 scores\n",
    "y = df_results_complete[df_results_complete['test']==1][labels].values.astype('float')\n",
    "y_bert = df_results_complete[df_results_complete['test']==1][labels_bert].values.astype('float')\n",
    "y_llm = df_results_complete[df_results_complete['test']==1][labels_llm].values.astype('float')\n",
    "\n",
    "extra = pd.DataFrame(\n",
    "    {'metric': ['f1_bert', 'prec_bert', 'recall_bert', 'f1_llm', 'prec_llm', 'recall_llm'], \n",
    "     'n': [len(df_results_complete)]*6, \n",
    "     'pct': [\n",
    "         f1_score(y, y_bert, average='micro'),\n",
    "         precision_score(y, y_bert, average='micro'),\n",
    "         recall_score(y, y_bert, average='micro'),\n",
    "         f1_score(y, y_llm, average='micro'),\n",
    "         precision_score(y, y_llm, average='micro'),\n",
    "         recall_score(y, y_llm, average='micro')\n",
    "     ]\n",
    "    }\n",
    "     )\n",
    "\n",
    "df_summary_test = pd.concat([df_summary_test, extra])\n",
    "df_summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5295b9f-015e-4b24-83ce-eddc25a1c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy to read results \n",
    "def create_legible(row, labels):\n",
    "    options = row[labels]\n",
    "    # find where true\n",
    "    out = ', '.join([x.removesuffix('_bert').removesuffix('_llm') for x in options[options==1].index])\n",
    "    if len(out)==0:\n",
    "        out = \"Other\"\n",
    "    return out\n",
    "\n",
    "df_results_complete['actual'] = df_results_complete.apply(create_legible, axis=1, args=(labels,))\n",
    "df_results_complete['bert'] = df_results_complete.apply(create_legible, axis=1, args=(labels_bert,))\n",
    "df_results_complete['llm'] = df_results_complete.apply(create_legible, axis=1, args=(labels_llm,))\n",
    "df_results_complete = df_results_complete[['response', 'actual', 'bert', 'llm'] + list(df_results_complete.columns[1:-3])]\n",
    "\n",
    "df_results_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028ed40-d88a-4f5d-99dd-9857d4145cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results and summary for later consumption\n",
    "df_results_complete.to_csv(out_folder+'/rbcm_q7_results.csv', index=False)\n",
    "df_summary.to_csv(out_folder+'/rbcm_q7_summary.csv', index=False)\n",
    "df_summary_test.to_csv(out_folder+'/rbcm_q7_summary_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779d492-9559-4ae2-971c-302f51bb3e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
