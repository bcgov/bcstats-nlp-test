{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92ae564-5032-42b2-902b-4e3b6042f753",
   "metadata": {},
   "source": [
    "Copyright 2025 Province of British Columbia\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a06ec1-84a6-45e5-b098-5e131f508b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system stuff\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# the usual\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cluster stuff \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan # package for density based clustering \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# human friendly topics\n",
    "from transformers import pipeline\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.manifold import TSNE # good for visuals\n",
    "import umap\n",
    "\n",
    "# display stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# my stuff\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.config import data_path_rvm, out_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde61eb6-5dc7-4827-b021-49caf97b5874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_path_rvm, sheet_name = 'Q07a')\n",
    "df = df.iloc[:, 4:-1]\n",
    "df.columns = ['Response'] + list(df.columns[1:])\n",
    "labels_original = list(df.columns)[1:]\n",
    "df.columns = [x.lower().replace(' ','_').replace('/','_').replace(':','_') for x in df.columns]\n",
    "df = df[~pd.isna(df.response)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e9a5c-6815-49fd-9346-baffd103c083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112813c9-8a60-46c4-987f-e297a6862977",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90138d-4500-47fd-91f8-f2d69f41466b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load an embedding model to translate the text to vectors\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# generate embeddings\n",
    "embeddings = embedding_model.encode(df.response, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5c270-b13d-4c50-80a1-2776ec7013f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize embeddings for better clustering results\n",
    "scaler = StandardScaler()\n",
    "normalized_embeddings = scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362da63-c6c9-4f0c-a196-744486580c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reduce dimensionality before clustering\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_components=10,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "reduced_embeddings = umap_reducer.fit_transform(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a866ae-23f5-4793-ae09-9ba56096ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ef49f-3eca-49ca-95c5-efa023d69ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run a density based clustering algorithm\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    min_samples=10,\n",
    "    cluster_selection_epsilon=0.3,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    core_dist_n_jobs=-1\n",
    ")\n",
    "\n",
    "df['cluster'] = clusterer.fit_predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1c861-9a48-47b8-ae40-7c3b0b1020d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a1b81-62c8-4db5-9c5f-05898dd60a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.cluster.value_counts().reset_index().sort_values(by='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c6805-a2f8-45d5-aea2-a20030503b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check them out in 2D space using TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings_tsne = tsne.fit_transform(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f942d1-f5d5-4b63-9eb3-2dd0352a38b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "df_plot = pd.DataFrame(reduced_embeddings_tsne, columns = ['x','y'])\n",
    "df_plot['cats'] = df.cluster.astype('category')\n",
    "\n",
    "ax = sns.scatterplot(df_plot, x='x', y='y', hue='cats', alpha=0.7, palette='Set2',legend=False)\n",
    "#sns.move_legend(ax, 'upper center', bbox_to_anchor=(0.5,-0.1), ncol=8, title=None, frameon=False)\n",
    "plt.savefig(out_folder+'/cluster_display.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b443d-6e35-495f-9c52-d136b021b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some human legible cluster names \n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a260c-ca51-4b37-b564-309894cecc9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def truncate_text(text, max_model_length=1023):\n",
    "    tokenized_text = summarizer.tokenizer(text, return_tensors='pt', truncation=True, max_length=max_model_length)\n",
    "    return summarizer.tokenizer.decode(tokenized_text['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "def create_cluster_labels_llm(\n",
    "    df, \n",
    "    chunk_size=50, \n",
    "    max_model_length=1023 # truncate texts that are too long\n",
    "):\n",
    "    cluster_labels = {}\n",
    "    n_clusters = len(df['cluster'].unique())\n",
    "    for cluster_i, cluster in enumerate(df['cluster'].unique()):\n",
    "        # get only the responses associated with a given cluster\n",
    "        cluster_texts = df[df['cluster']==cluster]['response'].tolist()\n",
    "        \n",
    "        chunk_summaries = []\n",
    "        # LLMs typically have 1024-2048 char limits, so chunking each cluster and then summarizing the summaries\n",
    "        n_texts = len(cluster_texts)\n",
    "        n_chunks = int(n_texts/chunk_size) + 1\n",
    "        for chunk_i, i in enumerate(range(0, len(cluster_texts), chunk_size)):\n",
    "            print(f'Summarizing Cluster {cluster_i+1:03,}/{n_clusters:03,} --- Completing Chunk {chunk_i+1:04,}/{n_chunks:04,}', end='\\r')\n",
    "            chunk = \" \".join(cluster_texts[i: i+chunk_size])\n",
    "            summary = summarizer(truncate_text(chunk, max_model_length), max_length=30, min_length=1, do_sample=False)[0]['summary_text']\n",
    "            chunk_summaries.append(summary)\n",
    "            \n",
    "        \n",
    "        final_summary = summarizer(\n",
    "            truncate_text(\" \".join(chunk_summaries)), \n",
    "            max_length=20, min_length=1, do_sample=False)[0]['summary_text']\n",
    "        cluster_labels[cluster] = final_summary\n",
    "        \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717aed17-e3c0-4b73-8679-8bab333d870a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels = create_cluster_labels_llm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceabe6a-59d4-401e-9173-58e04a755246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cluster_label'] = df['cluster'].map(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15423c-ba88-4967-9c32-2b2f44c057d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keyword method of creating topics\n",
    "\n",
    "# Load KeyBERT for keyword extraction\n",
    "kw_model = KeyBERT(embedding_model)\n",
    "\n",
    "def create_cluster_labels_keyword(\n",
    "    df, \n",
    "    chunk_size=50, \n",
    "    max_model_length=1023 # truncate texts that are too long\n",
    "):\n",
    "    cluster_labels = {}\n",
    "    n_clusters = len(df['cluster'].unique())\n",
    "    for cluster_i, cluster in enumerate(df['cluster'].unique()):\n",
    "        print(f'Completing Cluster {cluster_i:03,}/{n_clusters:03,}', end='\\r')\n",
    "        # get only the responses associated with a given cluster\n",
    "        cluster_texts = df[df['cluster']==cluster]['response'].tolist()\n",
    "        \n",
    "        joined_text = \" \".join(cluster_texts)\n",
    "        keywords = kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1,2), stop_words='english', top_n=5)\n",
    "        cluster_labels[cluster] = \", \".join([kw[0] for kw in keywords])\n",
    "        \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae7dcb-e65d-42a6-b676-a4b78024de20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels = create_cluster_labels_keyword(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0d3c8-0cef-47d7-b975-4c96b53f66a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cluster_keyword'] = df['cluster'].map(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8827a-0e37-43ba-98a6-1209a4e92668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(['cluster', 'cluster_label', 'cluster_keyword']).apply(lambda x: x.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8d1bc-a524-4762-aac3-8709034af879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cluster results for later viewing\n",
    "df.sort_values(by='cluster').to_csv(out_folder+'/rbcm_q7_cluster_results.csv', index=False)\n",
    "(df\n",
    " .groupby(['cluster', 'cluster_label', 'cluster_keyword'])\n",
    " .response.count()\n",
    " .reset_index()\n",
    " .sort_values(by='response', ascending=False)\n",
    " .to_csv(out_folder+'/rbcm_q7_cluster_counts.csv', index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b40838-07b3-4643-b505-07208465b46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
